El álgebra tensorial es una rama del álgebra que extiende los conceptos del álgebra lineal. Además, el concepto de tensor se traslada a la implementación de técnicas de aprendizaje automático, puesto que la aplicación que se realiza de ellos permite hacer más eficientes los cómputos de operaciones respecto al uso exclusivo de matrices.
La fuente principal de este capítulo es \textcite[capítulo 8]{treil2013}.

\section{Espacios duales}\label{espacios-duales}

En esta sección introducimos los conceptos esenciales sobre duales de espacios vectoriales, necesarios para llegar a la definición de tensor.

\subsection{Funcionales lineales y el espacio
dual}\label{funcionales-lineales-y-el-espacio-dual}

\defineb
Un \emph{funcional lineal} en un espacio vectorial finito dimensional
\(V\) sobre un cuerpo \(\KK\) es una aplicación lineal
\(L:V\rightarrow \KK\). \definee
\defineb
El \emph{espacio dual} de un espacio vectorial finito dimensional
\(V\) es \(V^*=\{L:V\rightarrow \KK\text{ lineal}\}=\mathcal{L}(V, \KK)\).
\definee

\exampleb
Sea \(V=\mathbb R^n\) y consideramos
\((\mathbb R^n)^*=\left\{L:\mathbb R^n\rightarrow\mathbb R\text{ lineal}\right\}\).
Sabemos que toda aplicación lineal de \(\mathbb{R}^{n}\) en
\(\mathbb{R}^{m}\) se expresa, fijada una base, como una matriz
\(m\times n\), luego identificamos \((\mathbb{R}^{n})^{*}\) con matrices
\(1\times n\) (en la base usual). Evidentemente hay un isomorfismo entre
este conjunto y \(\mathbb{R}^{n}\):

\begin{align*}
  (\mathbb{R}^{n})^{*} \cong \mathcal{M}_{1\times n}(\mathbb{R})&\cong \mathbb{R}^{n} \\
  (m_{1} \dots m_{n}) &\mapsto (m_{1}, \dots m_{n})
\end{align*}

Este hecho se generaliza para cualquier cuerpo \(\KK\):
\((\KK^{n})^{*}\cong \KK^{n}\). \examplee

\subsubsection{Cambio de coordenadas}\label{cambio-de-coordenadas}

Sea \(V\) \(\KK\)-espacio vectorial, sean
\(A=\left\{a_{1}, \dots a_{n}\right\}, B=\left\{b_{1}, \dots b_{n}\right\}\)
bases de \(V\) donde \(n = \dim_{\KK}V\).

Introducimos la siguiente notación: dada una base \(B\) de \(V\), \(B'\)
de \(W\), \(L\in\mathcal L(V, W)\), notaremos \([L]_{B', B}\) a la
expresión matricial de \(L\) en las bases \(B, B'\). Si \(L\in V^{*}\)
notamos \([L]_{B}\).

Sabemos que la expresión de \(L\in V^{*}\) en la base \(B\) viene dada
por su imagen por los vectores de la base, y el cambio de coordenadas es
\([L]_B=[L]_A[I]_{A,B}\).

Recordamos también que el cambio de base de \(v\in V\) se realiza
mediante \([B]_B=[I]_{B,A}[V]_A\) y que \([I]_{B,A}=[I]_{A,B}^{-1}\).
Así, llamando \(S=[I]_{B,A}\) observamos que el cambio de base de los
vectores asociados a las filas \([L]_B, [L]_A\) es:

\[
  [L]_{B}^t=(S^{-1})^t[L]_{A}^t
\]

\begin{prop}
Dado $V$ espacio vectorial, si $S$ es la matriz de cambio de base de $A$ a $B$ entonces la matriz de cambio de base de $V^{*}$ es $(S^{-1})^t$.
\end{prop}

\begin{lemma}
  \label{lemma:vectorcero}
  Sea $v\in V$. Si $L(v)=0\forall L\in V^{*}$ entonces $v=0$. Como consecuencia, si $L(v_1)=L(v_2)\forall L\in V^{*}$ entonces $v_1=v_2$.
  
  \begin{proof}
    Sea $B$ base de $V$. Entonces $L(v)=[L]_B[v]_B$. Basta tomar $L_k=[0, \dots 0, \overset{(k)}{1}, 0, \dots 0]$ y comprobar que en ese caso $L_k[v]_B=0$ implica que la $k$-ésima coordenada de $[v]_B$ es 0. Repitiendo el mismo paso para cada $k$ tenemos que $v=0$.
  \end{proof}
\end{lemma}

\subsection{El segundo dual}\label{el-segundo-dual}

Puesto que \(V^{*}\) es un espacio vectorial, podemos considerar también
su dual, que notaremos \(V^{**}\). Comprobaremos que, de hecho,
\(V^{**}\) es isomorfo a \(V\) de una forma natural. Dado \(v\in V\)
podemos tomar \(L_{v}\in V^{**}\) dado por
\(L_{v}(f) = f(v)\forall f\in V^{*}\). Así, podemos construir una
aplicación del espacio \(V\) en su segundo dual,
\(T:V\rightarrow V^{**}\), dada de forma natural por
\(Tv=L_v\forall v\in V\).

Para ver que \(T\) es un isomorfismo, observamos que las dimensiones de
los espacios coinciden: \(\dim V^{**}=\dim V^{*}=\dim V\). Por tanto,
bastará con ver que \(T\) es inyectivo: veamos para ello que
\(\Ker T=\{0\}\). Dado \(v\in \Ker T\), tenemos que
\(\forall f\in V^{*} f(v)=L_v(f)=T(v)(f)=0\). Por el
\autoref{lemma:vectorcero}, se tiene que \(v=0\).

Nótese que el isomorfismo \(T\) no depende de la elección de una base en
\(V\).

\section{Funciones multilineales.
Tensores}\label{funciones-multilineales.-tensores}

\defineb

Sean \(V_1,\dots,V_p,V\) espacios vectoriales sobre un cuerpo \(\KK\).
Una \emph{aplicación multilineal} (\(p\)-lineal) con valores en V es una
función \(F:V_1\times \dots\times V_p\rightarrow V\), lineal en cada
variable. Es decir, para cada \(k\in \{1,\dots, p\}\) y fijado
\((v_1,\dots,v_{k-1},0,v_{k+1},\dots,v_p)\in V_1\times \dots\times V_p\),
se tiene que la aplicación que lleva
\(v_k\mapsto F(v_1,\dots,v_{k-1},v_k,v_{k+1},\dots,v_p)\) es lineal.

Notamos por \(\LL(V_1,\dots,V_p;V)\) a la familia de todas las
aplicaciones \(p\)-lineales de \(V_1\times \dots\times V_p\) en \(V\).

\definee

\defineb

Un \emph{tensor} o \emph{funcional multilineal} es una aplicación
multilineal con codominio \(\KK\),
\(F:V_1\times \dots\times V_p\rightarrow \KK\). La cantidad \(p\) se
denomina el \emph{rango} o \emph{valencia} del tensor.

\definee

En particular, un tensor de rango 1 es un funcional lineal, y un
tensor de rango 2 es una forma bilineal.

\exampleb

Sean \(V_1,\dots,V_p\) \(\KK\)-espacios vectoriales y sean
\(f_1\in V_1^{*},\dots f_p\in V_p^{*}\) funcionales lineales. Definimos
el funcional multilineal \(F:V_1\times\dots\times V_p\rightarrow\KK\)
dado por
\[F(v_1,\dots,v_p)=f_1(v_1)f_2(v_2)\dots f_p(v_p),\ v_i\in V_i,\ k=1,2,\dots,p.\]

El funcional \(F\) se denomina \emph{producto tensorial} de los
funcionales \(f_i\) y lo notamos
\(F=f_1\otimes f_2\otimes\dots\otimes f_p\).

\examplee

\remb

El conjunto de las aplicaciones multilineales es un \(\KK\)-espacio
vectorial, mediante las siguientes operaciones de suma y producto por
escalar: sean \(F_1, F_2\in \LL(V_1,\dots,V_p;V),\alpha\in\KK\)

\begin{align*}
(F_1+F_2)(v_1,\dots,v_p)&=F_1(v_1,\dots,v_p)+F_2(v_1,\dots,v_p),\\
(\alpha F_1)(v_1,\dots,v_p)&=\alpha F_1(v_1,\dots,v_p).
\end{align*}

\reme

\propb
\label{prop:base-funcionales} Sean \(V_1,\dots,V_p\) \(\KK\)-espacios
vectoriales con bases \(B^{(1)},\dots,B^{(p)}\) respectivamente. Notamos
\(b_i^{(k)}\) al \(i\)-ésimo elemento de la base \(B^{(k)}\).

Para cada \(k\in\{1,\dots,p\}\) y para cada \(i\in\{1,\dots,\dim V_k\}\)
sea \(f_{i}^{(k)}\) el funcional lineal de \(V_k^{*}\) definido por

\begin{align*}
    f_{i}^{(k)}(b_i^{(k)})&=1\\
    f_{i}^{(k)}(b_j^{(k)})&=0,\ j\neq i.
\end{align*}

La familia
\[B=\left\{f_{i_1}^{(1)}\otimes\dots\otimes f_{i_p}^{(p)},\ 1\leq i_k\leq\dim{V_k},\ k\in\{1,\dots,p\}\right\}\]
es una base del espacio \(\LL(V_1,\dots, V_p;\KK)\).

En particular,
\[\dim\LL(V_1,\dots, V_p;\KK)=(\dim V_1)\dots (\dim V_p).\]

\proofb

Dada \(F\in \LL(V_1,\dots, V_p;\KK)\) queremos expresarla de forma única
en función de los elementos de la familia \(B\), es decir, buscamos
coeficientes \(\alpha_{i_1,i_2,\dots,i_p}\in\KK\) tales que

\begin{equation}
  \label{eq:basetensor}
  F=\sum\limits_{i_k\in\{1,\dots,\dim V_k\}} \alpha_{i_1,i_2,\dots,i_p}f_{i_1}^{(1)}\otimes\dots\otimes f_{i_p}^{(p)}\ .
\end{equation}

Por la definición de los funcionales, se tiene que
\begin{equation}
  \label{eq:evalbasetensor}
  f_{i_1}^{(1)}\otimes\dots\otimes f_{i_p}^{(p)}\left(b_{j_1}^{(1)},\dots,b_{j_p}^{(p)}\right)=1\Leftrightarrow i_1=j_1,\dots,i_p=j_p
\end{equation}
y, en otro caso,
\[f_{i_1}^{(1)}\otimes\dots\otimes f_{i_p}^{(p)}\left(b_{j_1}^{(1)},\dots,b_{j_p}^{(p)}\right)=0\ .\]

Evaluando ahora \(F\) \eqref{eq:basetensor} en
\(b_{i_1}^{(1)},\dots,b_{i_p}^{(p)}\):

\begin{equation*}
F\left(b_{i_1}^{(1)},\dots,b_{i_p}^{(p)}\right)=\alpha_{i_1,\dots,i_p}
\end{equation*}
lo cual nos da la unicidad de los coeficientes, en caso de que existan.
La existencia se deduce definiendo

\begin{equation*}
\alpha_{i_1,\dots,i_p}:=F\left(b_{i_1}^{(1)},\dots,b_{i_p}^{(p)}\right),
\end{equation*}
de forma que la condición \eqref{eq:evalbasetensor} se mantiene para
todas las tuplas del tipo \(b_{j_1}^{(1)},\dots,b_{j_p}^{(p)}\). Así, se
tiene la descomposición que buscamos y \(B\) es una base.

\proofe

\prope

\section{Productos tensoriales}\label{productos-tensoriales}

\defineb
Sean \(V_1,V_2,\dots,V_p\) espacios vectoriales. El producto tensorial
de los espacios es el conjunto de funcionales multilineales
\(\LL(V_1^*,V_2^*,\dots,V_p^*;\KK)\), y lo notamos
\(V_1\otimes V_2\otimes\dots\otimes V_p\). \definee

\corb\label{cor:base-tensor}
Sean \(V_1,\dots,V_p\) \(\KK\)-espacios vectoriales con bases
\(B^{(1)},\dots,B^{(p)}\) respectivamente. Llamamos \(b_i^{(k)}\) al
\(i\)-ésimo elemento de la base \(B^{(k)}\) y observamos que podemos
definir el producto tensorial de elementos de \(V_1,\dots,V_p\)
viéndolos como funcionales de \(V_1^*,\dots,V_p^*\). Entonces, la
familia
\[B=\left\{b_{i_1}^{(1)}\otimes\dots\otimes b_{i_p}^{(p)},\ 1\leq i_k\leq\dim{V_k},\ k\in\{1,\dots,p\}\right\},\]
es una base del espacio \(V_1\otimes V_2\otimes\dots\otimes V_p\).
\proofb
Consecuencia de la \autoref{prop:base-funcionales} y el
isomorfismo \(V_k^{**}\cong V_k\). \proofe
\core

\remb
Dados \(v_1\in V_1,\dots v_p\in V_p\), para
\(v'_k\in V_k,\ k\in\{1,\dots,p\},\lambda,\mu\in\KK\) y cualesquiera
\(f_1\in V_1^*,\dots f_p\in V_p^*\) se tiene:

\begin{align*}
  (v_1\otimes v_2\otimes\dots\otimes(\lambda v_k &+ \mu v'_k)\otimes\dots\otimes v_p)(f_1,\dots,f_p)=\\
  f_1(v_1)\dots f_k(\lambda v_k &+ \mu v'_k) \dots f_p(v_p) =\\
  f_1(v_1)\dots (\lambda f_k(v_k) &+ \mu f_k(v'_k)) \dots f_p(v_p) = \\
  \lambda f_1(v_1)\dots f_k(v_k) \dots f_p(v_p) &+ \mu \lambda f_1(v_1)\dots f_k(v'_k) \dots f_p(v_p) =\\
  (\lambda v_1\otimes v_2\otimes\dots\otimes v_k\otimes\dots\otimes v_p &+\mu v_1\otimes v_2\otimes\dots\otimes v'_k\otimes\dots\otimes)(f_1,\dots,f_p)
\end{align*}

Hemos comprobado que la aplicación
\((v_1,v_2,\dots,v_p)\mapsto v_1\otimes v_2\otimes\dots\otimes v_p\) es
lineal en cada variable. \reme

\section{Tensores covariantes y
contravariantes}\label{tensores-covariantes-y-contravariantes}

Sean \(X_1,X_2,\dots X_p,V\) espacios vectoriales y sea \(V_k\) bien
\(X_k\) o bien \(X_k^*\), para cada \(k=1,2,\dots,p\).

\defineb
Decimos que  \(F\in \LL(V_1,V_2,\dots,V_p;V)\)
es una aplicación multilineal \emph{covariante} en la \(k\)-ésima variable si \(V_k=X_k\) y
\emph{contravariante} en dicha variable si \(V_k=X_k^*\).

Si \(F\) es covariante (resp. contravariante) en todas las variables
decimos simplemente que es covariante (resp. contravariante). Si \(F\)
es covariante en \(r\) variables y contravariante en \(s\) variables,
decimos que es \(r\)-covariante \(s\)-contravariante, o de \emph{tipo}
\((r, s)\).\definee

\exampleb Algunos casos particulares de tensores, para observar que generalizan los objetos del álgebra lineal:
\begin{itemize}
\item Dado un espacio vectorial \(V\), un funcional \(f\in V^*\) es un
  tensor 1-covariante.
  \item Un vector \(v\in V\), visto en el doble dual
    \(V^{**}\), es un tensor 1-contravariante.
    \item Por convención, se dice que una constante $\lambda\in\KK$ es un tensor de tipo \((0,0)\).
\end{itemize}
\examplee

\section{Los tensores en aprendizaje
automático}\label{los-tensores-en-aprendizaje-automuxe1tico}

Se ha visto que los tensores generalizan estructuras como los vectores y las aplicaciones lineales. Es importante notar que dichos objetos se pueden representar mediante secuencias finitas de escalares, las componentes, que dependen de la base escogida para los espacios vectoriales donde se esté trabajando. De igual forma, un tensor se puede expresar en componentes del cuerpo $\KK$ respecto de una base.

La expresión de un tensor en componentes necesitará una representación en tantas dimensiones como su rango. Intuitivamente, podemos decir que el rango de un tensor es el número de índices necesarios para recorrer sus componentes.

Esta idea se lleva al aprendizaje automático como una generalización de las matrices. Esencialmente, se le llama \emph{tensor} de rango $p$ a un objeto $T\in \KK^{d_1d_2 \dots d_p}$. Aunque este es un caso particular de los tensores estudiados en este capítulo, no se suele hacer uso de sus propiedades algebraicas. Sin embargo, algunas de las operaciones y descomposiciones de cálculo con tensores permiten realizar un uso eficiente de la memoria disponible en la máquina a la hora de entrenar una red neuronal \autocite{kolda2009}.
